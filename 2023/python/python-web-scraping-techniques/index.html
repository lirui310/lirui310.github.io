<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Orion K">
    
    <title>
        
            Python网络爬虫技术：从入门到精通 |
        
        Orion K Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
        <link rel="shortcut icon" href="/images/logo.svg">
    
    
<link rel="stylesheet" href="/font/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/font/css/regular.min.css">

    
<link rel="stylesheet" href="/font/css/solid.min.css">

    
<link rel="stylesheet" href="/font/css/brands.min.css">

    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"zh-CN","path":"search.json"}
    KEEP.theme_config = {"base_info":{"primary_color":"#0066cc","title":"Orion K Blog","author":"Orion K","avatar":"/images/avatar.svg","logo":"/images/logo.svg","favicon":"/images/logo.svg"},"menu":{"home":"/                        || fa-solid fa-home","archives":"/archives            || fa-solid fa-box-archive","categories":"/categories        || fa-solid fa-layer-group"},"first_screen":{"enable":false,"background_img":"/images/bg.svg","background_img_dark":"/images/bg.svg","description":"Keep writing and Keep loving.","hitokoto":false},"social_contact":{"enable":false,"links":{"github":null,"weixin":null,"qq":null,"weibo":null,"zhihu":null,"twitter":null,"x":null,"facebook":null,"email":null}},"scroll":{"progress_bar":false,"percent":false,"hide_header":true},"home":{"announcement":"欢迎来到 Orion K 的博客，这是一个分享互联网技术的博客。联系：wxmm686800@gmail.com","category":true,"tag":true,"post_datetime":"created"},"post":{"author_badge":{"enable":true,"level_badge":true,"custom_badge":["One","Two","Three"]},"word_count":{"wordcount":false,"min2read":false},"datetime_format":"YYYY-MM-DD HH:mm:ss","copyright_info":false,"share":false,"reward":{"enable":false,"img_link":null,"text":null,"icon":null}},"code_block":{"tools":{"enable":false,"style":"default"},"highlight_theme":"default"},"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true,"layout":"right"},"website_count":{"busuanzi_count":{"enable":false,"site_uv":false,"site_pv":false,"page_pv":false}},"local_search":{"enable":true,"preload":true},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.39"},"waline":{"server_url":null,"reaction":false,"version":"3.3.2"},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false},"artalk":{"server":null},"disqus":{"shortname":null}},"rss":{"enable":false},"lazyload":{"enable":false},"cdn":{"enable":false,"provider":"cdnjs"},"pjax":{"enable":false},"footer":{"since":2025,"word_count":false,"site_deploy":{"enable":true,"provider":"github","url":null},"record":{"enable":false,"list":[{"code":null,"link":null}]}},"inject":{"enable":false,"css":[null],"js":[null]},"root":"","source_data":{},"version":"4.2.5"}
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"}
    KEEP.language_code_block = {"copy":"复制代码","copied":"已复制","fold":"折叠代码块","folded":"已折叠"}
    KEEP.language_copy_copyright = {"copy":"复制版权信息","copied":"已复制","title":"原文标题","author":"原文作者","link":"原文链接"}
  </script>
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
</div>



<main class="page-container border-box">
    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="border-box header-content">
        <div class="left flex-start border-box">
            
                <a class="logo-image border-box" href="/">
                    <img src="/images/logo.svg">
                </a>
            
            <a class="site-name border-box" href="/">
               Orion K Blog
            </a>
        </div>

        <div class="right border-box">
            <div class="pc border-box">
                <ul class="menu-list border-box">
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-home"></i>
                                
                                首页
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/archives">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-box-archive"></i>
                                
                                归档
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/categories">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-layer-group"></i>
                                
                                分类
                                
                            </a>
                            
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="menu-text-color fas search fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile border-box flex-start">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list border-box">
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-home"></i>
                                </span>
                            
                            首页
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/archives">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-box-archive"></i>
                                </span>
                            
                            归档
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/categories">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-layer-group"></i>
                                </span>
                            
                            分类
                        </a>
                        
                    </label>
                    
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">
                

                    
<div class="fade-in-down-animation">
    <div class="post-page-container border-box">
        <div class="post-content-container border-box">
            

            <div class="post-content-bottom border-box">
                
                    <div class="post-title">
                        Python网络爬虫技术：从入门到精通
                    </div>
                

                
                    <div class="post-header border-box">
                        
                            <div class="avatar-box border-box">
                                <img src="/images/avatar.svg">
                            </div>
                        
                        <div class="info-box">
                            <div class="author border-box">
                                <span class="name">Orion K</span>
                                
                                    <span class="author-badge">Lv6</span>
                                
                            </div>
                            <div class="meta-info border-box">
                                

<div class="post-meta-info-container border-box post">
    <div class="post-meta-info border-box">
        

        
            <span class="meta-info-item post-create-date">
                <i class="icon fa-solid fa-calendar-plus"></i>&nbsp;
                <span class="datetime">2023-01-08 16:20:00</span>
            </span>
        

        
            <span class="meta-info-item post-category border-box"><i class="icon fas fa-folder"></i>&nbsp;
                <ul class="post-category-ul">
                    
                            <li class="category-item"><a href="/categories/python/">python</a></li>
                        
                    
                </ul>
            </span>
        

        
            <span class="post-tag meta-info-item border-box">
                <ul class="post-tag-ul">
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></li>
                        
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/">数据采集</a></li>
                        
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/requests/">requests</a></li>
                        
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/BeautifulSoup/">BeautifulSoup</a></li>
                        
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/Scrapy/">Scrapy</a></li>
                        
                    
                </ul>
            </span>
        

        
        
        
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="post-content keep-markdown-body ">
                    

                    
                         <h1 id="Python网络爬虫技术：从入门到精通"><a href="#Python网络爬虫技术：从入门到精通" class="headerlink" title="Python网络爬虫技术：从入门到精通"></a>Python网络爬虫技术：从入门到精通</h1><p>网络爬虫是Python最流行的应用领域之一，它允许我们自动化地从网站获取数据。无论是数据分析、机器学习还是自动化任务，网络爬虫都是一项非常有用的技能。在这篇文章中，我将带你从基础到高级，全面掌握Python网络爬虫技术。</p>
<h2 id="网络爬虫基础"><a href="#网络爬虫基础" class="headerlink" title="网络爬虫基础"></a>网络爬虫基础</h2><h3 id="什么是网络爬虫？"><a href="#什么是网络爬虫？" class="headerlink" title="什么是网络爬虫？"></a>什么是网络爬虫？</h3><p>网络爬虫（Web Scraping）是一种通过程序自动获取网页内容的技术。它可以模拟人类浏览网页的行为，访问网站，提取数据，并将其保存为结构化格式。</p>
<h3 id="爬虫的合法性和道德考量"><a href="#爬虫的合法性和道德考量" class="headerlink" title="爬虫的合法性和道德考量"></a>爬虫的合法性和道德考量</h3><p>在开始爬取网站之前，需要考虑以下几点：</p>
<ol>
<li><strong>查看robots.txt文件</strong>：这个文件定义了网站允许爬虫访问的部分</li>
<li><strong>遵守网站的使用条款</strong>：某些网站明确禁止爬虫</li>
<li><strong>控制请求频率</strong>：过于频繁的请求可能会对网站造成负担</li>
<li><strong>尊重版权</strong>：获取的数据可能受版权保护</li>
<li><strong>考虑使用官方API</strong>：如果网站提供API，优先使用API而不是爬虫</li>
</ol>
<h3 id="网络爬虫的基本流程"><a href="#网络爬虫的基本流程" class="headerlink" title="网络爬虫的基本流程"></a>网络爬虫的基本流程</h3><ol>
<li>发送HTTP请求获取网页内容</li>
<li>解析HTML或XML提取所需数据</li>
<li>处理和存储数据</li>
<li>根据需要继续爬取其他页面</li>
</ol>
<h2 id="基本爬虫工具"><a href="#基本爬虫工具" class="headerlink" title="基本爬虫工具"></a>基本爬虫工具</h2><h3 id="Requests库"><a href="#Requests库" class="headerlink" title="Requests库"></a>Requests库</h3><p><code>requests</code>是Python最流行的HTTP客户端库，它使发送HTTP请求变得简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送GET请求</span></span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查状态码</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;状态码: <span class="subst">&#123;response.status_code&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看响应内容</span></span><br><span class="line"><span class="built_in">print</span>(response.text[:<span class="number">100</span>])  <span class="comment"># 打印前100个字符</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送带参数的GET请求</span></span><br><span class="line">params = &#123;<span class="string">&#x27;q&#x27;</span>: <span class="string">&#x27;python&#x27;</span>, <span class="string">&#x27;page&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com/search&#x27;</span>, params=params)</span><br><span class="line"><span class="built_in">print</span>(response.url)  <span class="comment"># 打印完整URL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送POST请求</span></span><br><span class="line">data = &#123;<span class="string">&#x27;username&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;pass&#x27;</span>&#125;</span><br><span class="line">response = requests.post(<span class="string">&#x27;https://www.example.com/login&#x27;</span>, data=data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理JSON响应</span></span><br><span class="line">response = requests.get(<span class="string">&#x27;https://api.github.com/users/python&#x27;</span>)</span><br><span class="line">user_data = response.json()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;GitHub用户名: <span class="subst">&#123;user_data[<span class="string">&#x27;login&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;仓库数量: <span class="subst">&#123;user_data[<span class="string">&#x27;public_repos&#x27;</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="BeautifulSoup库"><a href="#BeautifulSoup库" class="headerlink" title="BeautifulSoup库"></a>BeautifulSoup库</h3><p><code>BeautifulSoup</code>是一个强大的HTML和XML解析库，它可以帮助我们从网页中提取数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取网页内容</span></span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com&#x27;</span>)</span><br><span class="line">html_content = response.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建BeautifulSoup对象</span></span><br><span class="line">soup = BeautifulSoup(html_content, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找元素</span></span><br><span class="line">title = soup.title</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;页面标题: <span class="subst">&#123;title.string&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找所有链接</span></span><br><span class="line">links = soup.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> links[:<span class="number">5</span>]:  <span class="comment"># 打印前5个链接</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;链接文本: <span class="subst">&#123;link.text&#125;</span>, URL: <span class="subst">&#123;link.get(<span class="string">&#x27;href&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用CSS选择器</span></span><br><span class="line">main_content = soup.select(<span class="string">&#x27;div.main-content&#x27;</span>)</span><br><span class="line">headings = soup.select(<span class="string">&#x27;h1, h2, h3&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取特定元素</span></span><br><span class="line">article = soup.find(<span class="string">&#x27;article&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> article:</span><br><span class="line">    article_title = article.find(<span class="string">&#x27;h1&#x27;</span>).text</span><br><span class="line">    article_paragraphs = article.find_all(<span class="string">&#x27;p&#x27;</span>)</span><br><span class="line">    article_text = <span class="string">&#x27;\n&#x27;</span>.join([p.text <span class="keyword">for</span> p <span class="keyword">in</span> article_paragraphs])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;文章标题: <span class="subst">&#123;article_title&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;文章内容: <span class="subst">&#123;article_text[:<span class="number">200</span>]&#125;</span>...&quot;</span>)  <span class="comment"># 打印前200个字符</span></span><br></pre></td></tr></table></figure>

<h3 id="lxml库"><a href="#lxml库" class="headerlink" title="lxml库"></a>lxml库</h3><p><code>lxml</code>是一个高性能的HTML和XML解析库，它比BeautifulSoup更快，但API不太友好：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取网页内容</span></span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com&#x27;</span>)</span><br><span class="line">html_content = response.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析HTML</span></span><br><span class="line">html = etree.HTML(html_content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用XPath提取数据</span></span><br><span class="line">title = html.xpath(<span class="string">&#x27;//title/text()&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;页面标题: <span class="subst">&#123;title[<span class="number">0</span>] <span class="keyword">if</span> title <span class="keyword">else</span> <span class="string">&#x27;No title&#x27;</span>&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取所有链接</span></span><br><span class="line">links = html.xpath(<span class="string">&#x27;//a/@href&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> links[:<span class="number">5</span>]:  <span class="comment"># 打印前5个链接</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;链接: <span class="subst">&#123;link&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取特定元素</span></span><br><span class="line">articles = html.xpath(<span class="string">&#x27;//article&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> article <span class="keyword">in</span> articles:</span><br><span class="line">    article_title = article.xpath(<span class="string">&#x27;.//h1/text()&#x27;</span>)</span><br><span class="line">    article_content = article.xpath(<span class="string">&#x27;.//p/text()&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;文章标题: <span class="subst">&#123;article_title[<span class="number">0</span>] <span class="keyword">if</span> article_title <span class="keyword">else</span> <span class="string">&#x27;No title&#x27;</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;文章内容: <span class="subst">&#123;<span class="string">&#x27;&#x27;</span>.join(article_content)[:<span class="number">200</span>]&#125;</span>...&quot;</span>)  <span class="comment"># 打印前200个字符</span></span><br></pre></td></tr></table></figure>

<h2 id="中级爬虫技术"><a href="#中级爬虫技术" class="headerlink" title="中级爬虫技术"></a>中级爬虫技术</h2><h3 id="处理表单和登录"><a href="#处理表单和登录" class="headerlink" title="处理表单和登录"></a>处理表单和登录</h3><p>许多网站需要登录才能访问内容。以下是如何使用<code>requests</code>处理登录：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话对象</span></span><br><span class="line">session = requests.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取登录页面（可能包含CSRF令牌）</span></span><br><span class="line">login_url = <span class="string">&#x27;https://www.example.com/login&#x27;</span></span><br><span class="line">response = session.get(login_url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们需要从页面提取CSRF令牌</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">csrf_token = soup.find(<span class="string">&#x27;input&#x27;</span>, &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;csrf_token&#x27;</span>&#125;)[<span class="string">&#x27;value&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备登录数据</span></span><br><span class="line">login_data = &#123;</span><br><span class="line">    <span class="string">&#x27;username&#x27;</span>: <span class="string">&#x27;your_username&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;your_password&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;csrf_token&#x27;</span>: csrf_token</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送登录请求</span></span><br><span class="line">response = session.post(login_url, data=login_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否登录成功</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;Welcome&#x27;</span> <span class="keyword">in</span> response.text <span class="keyword">or</span> response.url != login_url:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;登录成功!&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 访问需要登录的页面</span></span><br><span class="line">    protected_url = <span class="string">&#x27;https://www.example.com/dashboard&#x27;</span></span><br><span class="line">    response = session.get(protected_url)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Dashboard页面标题: <span class="subst">&#123;BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>).title.string&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;登录失败!&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="处理Cookie"><a href="#处理Cookie" class="headerlink" title="处理Cookie"></a>处理Cookie</h3><p>Cookie对于维护会话状态很重要：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动设置Cookie</span></span><br><span class="line">cookies = &#123;<span class="string">&#x27;session_id&#x27;</span>: <span class="string">&#x27;12345&#x27;</span>, <span class="string">&#x27;user_id&#x27;</span>: <span class="string">&#x27;67890&#x27;</span>&#125;</span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com&#x27;</span>, cookies=cookies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从响应中获取Cookie</span></span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.cookies[<span class="string">&#x27;session_id&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用会话自动处理Cookie</span></span><br><span class="line">session = requests.Session()</span><br><span class="line">session.get(<span class="string">&#x27;https://www.example.com&#x27;</span>)  <span class="comment"># 这将设置Cookie</span></span><br><span class="line">response = session.get(<span class="string">&#x27;https://www.example.com/profile&#x27;</span>)  <span class="comment"># 使用之前设置的Cookie</span></span><br></pre></td></tr></table></figure>

<h3 id="处理JavaScript渲染的页面"><a href="#处理JavaScript渲染的页面" class="headerlink" title="处理JavaScript渲染的页面"></a>处理JavaScript渲染的页面</h3><p>许多现代网站使用JavaScript动态加载内容，这对传统爬虫构成了挑战。</p>
<h4 id="使用Selenium"><a href="#使用Selenium" class="headerlink" title="使用Selenium"></a>使用Selenium</h4><p>Selenium可以自动化浏览器，执行JavaScript：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.service <span class="keyword">import</span> Service</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"><span class="keyword">from</span> webdriver_manager.chrome <span class="keyword">import</span> ChromeDriverManager</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置Chrome选项</span></span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">&quot;--headless&quot;</span>)  <span class="comment"># 无头模式，不显示浏览器窗口</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化WebDriver</span></span><br><span class="line">driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问网页</span></span><br><span class="line">driver.get(<span class="string">&#x27;https://www.example.com&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待JavaScript执行</span></span><br><span class="line">time.sleep(<span class="number">2</span>)  <span class="comment"># 简单等待</span></span><br><span class="line"><span class="comment"># 或者使用显式等待</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line">WebDriverWait(driver, <span class="number">10</span>).until(</span><br><span class="line">    EC.presence_of_element_located((By.ID, <span class="string">&quot;dynamic-content&quot;</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取渲染后的页面内容</span></span><br><span class="line">html_content = driver.page_source</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取数据</span></span><br><span class="line">elements = driver.find_elements(By.CSS_SELECTOR, <span class="string">&#x27;.item&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> elements:</span><br><span class="line">    <span class="built_in">print</span>(element.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 与页面交互</span></span><br><span class="line">search_box = driver.find_element(By.NAME, <span class="string">&#x27;q&#x27;</span>)</span><br><span class="line">search_box.send_keys(<span class="string">&#x27;Python&#x27;</span>)</span><br><span class="line">search_box.submit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭浏览器</span></span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>

<h4 id="使用Requests-HTML"><a href="#使用Requests-HTML" class="headerlink" title="使用Requests-HTML"></a>使用Requests-HTML</h4><p><code>requests-html</code>是<code>requests</code>的扩展，支持JavaScript渲染：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests_html <span class="keyword">import</span> HTMLSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话</span></span><br><span class="line">session = HTMLSession()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取页面</span></span><br><span class="line">response = session.get(<span class="string">&#x27;https://www.example.com&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 渲染JavaScript</span></span><br><span class="line">response.html.render()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取数据</span></span><br><span class="line">elements = response.html.find(<span class="string">&#x27;.item&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> elements:</span><br><span class="line">    <span class="built_in">print</span>(element.text)</span><br></pre></td></tr></table></figure>

<h3 id="处理分页"><a href="#处理分页" class="headerlink" title="处理分页"></a>处理分页</h3><p>许多网站将内容分成多个页面，我们需要遍历这些页面：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&#x27;https://www.example.com/products?page=&#x27;</span></span><br><span class="line">all_products = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历前5页</span></span><br><span class="line"><span class="keyword">for</span> page_num <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    url = base_url + <span class="built_in">str</span>(page_num)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;爬取页面: <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    response = requests.get(url)</span><br><span class="line">    soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 提取产品信息</span></span><br><span class="line">    products = soup.select(<span class="string">&#x27;.product-item&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> product <span class="keyword">in</span> products:</span><br><span class="line">        product_name = product.select_one(<span class="string">&#x27;.product-name&#x27;</span>).text.strip()</span><br><span class="line">        product_price = product.select_one(<span class="string">&#x27;.product-price&#x27;</span>).text.strip()</span><br><span class="line">        all_products.append(&#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: product_name,</span><br><span class="line">            <span class="string">&#x27;price&#x27;</span>: product_price</span><br><span class="line">        &#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 可选：检查是否有下一页</span></span><br><span class="line">    next_button = soup.select_one(<span class="string">&#x27;.pagination .next&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> next_button <span class="keyword">or</span> <span class="string">&#x27;disabled&#x27;</span> <span class="keyword">in</span> next_button.get(<span class="string">&#x27;class&#x27;</span>, []):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;已到达最后一页&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;共爬取 <span class="subst">&#123;<span class="built_in">len</span>(all_products)&#125;</span> 个产品&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="高级爬虫技术"><a href="#高级爬虫技术" class="headerlink" title="高级爬虫技术"></a>高级爬虫技术</h2><h3 id="使用Scrapy框架"><a href="#使用Scrapy框架" class="headerlink" title="使用Scrapy框架"></a>使用Scrapy框架</h3><p>Scrapy是一个强大的爬虫框架，适合大规模爬虫项目：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装Scrapy</span></span><br><span class="line">pip install scrapy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建新项目</span></span><br><span class="line">scrapy startproject bookstore</span><br><span class="line"><span class="built_in">cd</span> bookstore</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建爬虫</span></span><br><span class="line">scrapy genspider books example.com</span><br></pre></td></tr></table></figure>

<p>编辑爬虫文件 <code>bookstore/spiders/books.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BooksSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;books&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;books.toscrape.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://books.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 提取所有书籍</span></span><br><span class="line">        books = response.css(<span class="string">&#x27;article.product_pod&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> book <span class="keyword">in</span> books:</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">&#x27;title&#x27;</span>: book.css(<span class="string">&#x27;h3 a::attr(title)&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;price&#x27;</span>: book.css(<span class="string">&#x27;p.price_color::text&#x27;</span>).get(),</span><br><span class="line">                <span class="string">&#x27;rating&#x27;</span>: book.css(<span class="string">&#x27;p.star-rating::attr(class)&#x27;</span>).get().split()[-<span class="number">1</span>]</span><br><span class="line">            &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 处理分页</span></span><br><span class="line">        next_page = response.css(<span class="string">&#x27;li.next a::attr(href)&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page:</span><br><span class="line">            <span class="keyword">yield</span> response.follow(next_page, <span class="variable language_">self</span>.parse)</span><br></pre></td></tr></table></figure>

<p>运行爬虫并保存结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl books -o books.json</span><br></pre></td></tr></table></figure>

<h3 id="处理反爬虫机制"><a href="#处理反爬虫机制" class="headerlink" title="处理反爬虫机制"></a>处理反爬虫机制</h3><p>网站通常会实施反爬虫措施，以下是一些应对策略：</p>
<h4 id="1-设置请求头"><a href="#1-设置请求头" class="headerlink" title="1. 设置请求头"></a>1. 设置请求头</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常见User-Agent列表</span></span><br><span class="line">user_agents = [</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0&#x27;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机选择User-Agent</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: random.choice(user_agents),</span><br><span class="line">    <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en-US,en;q=0.5&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://www.google.com/&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;DNT&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,  <span class="comment"># Do Not Track</span></span><br><span class="line">    <span class="string">&#x27;Connection&#x27;</span>: <span class="string">&#x27;keep-alive&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Upgrade-Insecure-Requests&#x27;</span>: <span class="string">&#x27;1&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com&#x27;</span>, headers=headers)</span><br></pre></td></tr></table></figure>

<h4 id="2-控制请求频率"><a href="#2-控制请求频率" class="headerlink" title="2. 控制请求频率"></a>2. 控制请求频率</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">urls = [<span class="string">&#x27;https://www.example.com/page1&#x27;</span>, <span class="string">&#x27;https://www.example.com/page2&#x27;</span>, <span class="string">&#x27;...&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">    response = requests.get(url)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;爬取 <span class="subst">&#123;url&#125;</span>, 状态码: <span class="subst">&#123;response.status_code&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机延迟1-5秒</span></span><br><span class="line">    delay = random.uniform(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;等待 <span class="subst">&#123;delay:<span class="number">.2</span>f&#125;</span> 秒...&quot;</span>)</span><br><span class="line">    time.sleep(delay)</span><br></pre></td></tr></table></figure>

<h4 id="3-使用代理"><a href="#3-使用代理" class="headerlink" title="3. 使用代理"></a>3. 使用代理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://10.10.10.10:8000&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;http://10.10.10.10:8000&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com&#x27;</span>, proxies=proxies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用代理池</span></span><br><span class="line">proxy_pool = [</span><br><span class="line">    &#123;<span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://proxy1.example.com:8000&#x27;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://proxy2.example.com:8000&#x27;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://proxy3.example.com:8000&#x27;</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com&#x27;</span>, proxies=random.choice(proxy_pool))</span><br></pre></td></tr></table></figure>

<h4 id="4-处理验证码"><a href="#4-处理验证码" class="headerlink" title="4. 处理验证码"></a>4. 处理验证码</h4><p>对于验证码，可以使用OCR库或验证码识别服务：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> pytesseract</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取验证码图片</span></span><br><span class="line">response = requests.get(<span class="string">&#x27;https://www.example.com/captcha.php&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(BytesIO(response.content))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用pytesseract识别验证码</span></span><br><span class="line">captcha_text = pytesseract.image_to_string(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;识别的验证码: <span class="subst">&#123;captcha_text&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交表单时包含验证码</span></span><br><span class="line">form_data = &#123;</span><br><span class="line">    <span class="string">&#x27;username&#x27;</span>: <span class="string">&#x27;user&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;pass&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;captcha&#x27;</span>: captcha_text</span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(<span class="string">&#x27;https://www.example.com/login&#x27;</span>, data=form_data)</span><br></pre></td></tr></table></figure>

<h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><p>爬取的数据需要妥善存储：</p>
<h4 id="1-保存为CSV"><a href="#1-保存为CSV" class="headerlink" title="1. 保存为CSV"></a>1. 保存为CSV</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据</span></span><br><span class="line">data = [</span><br><span class="line">    &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Product 1&#x27;</span>, <span class="string">&#x27;price&#x27;</span>: <span class="string">&#x27;$19.99&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>: <span class="string">&#x27;4.5&#x27;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Product 2&#x27;</span>, <span class="string">&#x27;price&#x27;</span>: <span class="string">&#x27;$29.99&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>: <span class="string">&#x27;3.8&#x27;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Product 3&#x27;</span>, <span class="string">&#x27;price&#x27;</span>: <span class="string">&#x27;$15.49&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>: <span class="string">&#x27;4.2&#x27;</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存为CSV</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;products.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    fieldnames = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>]</span><br><span class="line">    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)</span><br><span class="line">    </span><br><span class="line">    writer.writeheader()</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">        writer.writerow(item)</span><br></pre></td></tr></table></figure>

<h4 id="2-保存为JSON"><a href="#2-保存为JSON" class="headerlink" title="2. 保存为JSON"></a>2. 保存为JSON</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存为JSON</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;products.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> jsonfile:</span><br><span class="line">    json.dump(data, jsonfile, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-保存到数据库"><a href="#3-保存到数据库" class="headerlink" title="3. 保存到数据库"></a>3. 保存到数据库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接到SQLite数据库</span></span><br><span class="line">conn = sqlite3.connect(<span class="string">&#x27;products.db&#x27;</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line">cursor.execute(<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">CREATE TABLE IF NOT EXISTS products (</span></span><br><span class="line"><span class="string">    id INTEGER PRIMARY KEY,</span></span><br><span class="line"><span class="string">    name TEXT NOT NULL,</span></span><br><span class="line"><span class="string">    price TEXT NOT NULL,</span></span><br><span class="line"><span class="string">    rating TEXT NOT NULL</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 插入数据</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">    cursor.execute(</span><br><span class="line">        <span class="string">&#x27;INSERT INTO products (name, price, rating) VALUES (?, ?, ?)&#x27;</span>,</span><br><span class="line">        (item[<span class="string">&#x27;name&#x27;</span>], item[<span class="string">&#x27;price&#x27;</span>], item[<span class="string">&#x27;rating&#x27;</span>])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交更改并关闭连接</span></span><br><span class="line">conn.commit()</span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure>

<h2 id="实际爬虫案例"><a href="#实际爬虫案例" class="headerlink" title="实际爬虫案例"></a>实际爬虫案例</h2><h3 id="案例1：爬取新闻网站"><a href="#案例1：爬取新闻网站" class="headerlink" title="案例1：爬取新闻网站"></a>案例1：爬取新闻网站</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scrape_news</span>():</span><br><span class="line">    <span class="comment"># 目标URL</span></span><br><span class="line">    url = <span class="string">&#x27;https://news.example.com&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 发送请求</span></span><br><span class="line">    response = requests.get(url)</span><br><span class="line">    soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 提取新闻文章</span></span><br><span class="line">    articles = soup.select(<span class="string">&#x27;article.news-item&#x27;</span>)</span><br><span class="line">    news_data = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> article <span class="keyword">in</span> articles:</span><br><span class="line">        <span class="comment"># 提取标题</span></span><br><span class="line">        title = article.select_one(<span class="string">&#x27;h2.title&#x27;</span>).text.strip()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 提取链接</span></span><br><span class="line">        link = article.select_one(<span class="string">&#x27;a&#x27;</span>)[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> link.startswith(<span class="string">&#x27;http&#x27;</span>):</span><br><span class="line">            link = url + link</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 提取摘要</span></span><br><span class="line">        summary = article.select_one(<span class="string">&#x27;p.summary&#x27;</span>)</span><br><span class="line">        summary = summary.text.strip() <span class="keyword">if</span> summary <span class="keyword">else</span> <span class="string">&quot;无摘要&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 提取发布日期</span></span><br><span class="line">        date = article.select_one(<span class="string">&#x27;span.date&#x27;</span>)</span><br><span class="line">        date = date.text.strip() <span class="keyword">if</span> date <span class="keyword">else</span> <span class="string">&quot;未知日期&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 提取分类</span></span><br><span class="line">        category = article.select_one(<span class="string">&#x27;span.category&#x27;</span>)</span><br><span class="line">        category = category.text.strip() <span class="keyword">if</span> category <span class="keyword">else</span> <span class="string">&quot;未分类&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加到数据列表</span></span><br><span class="line">        news_data.append(&#123;</span><br><span class="line">            <span class="string">&#x27;title&#x27;</span>: title,</span><br><span class="line">            <span class="string">&#x27;link&#x27;</span>: link,</span><br><span class="line">            <span class="string">&#x27;summary&#x27;</span>: summary,</span><br><span class="line">            <span class="string">&#x27;date&#x27;</span>: date,</span><br><span class="line">            <span class="string">&#x27;category&#x27;</span>: category,</span><br><span class="line">            <span class="string">&#x27;scraped_at&#x27;</span>: datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存数据</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;news.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">        fieldnames = [<span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;link&#x27;</span>, <span class="string">&#x27;summary&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;category&#x27;</span>, <span class="string">&#x27;scraped_at&#x27;</span>]</span><br><span class="line">        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)</span><br><span class="line">        writer.writeheader()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> news_data:</span><br><span class="line">            writer.writerow(item)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;已爬取 <span class="subst">&#123;<span class="built_in">len</span>(news_data)&#125;</span> 条新闻并保存到 news.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    scrape_news()</span><br></pre></td></tr></table></figure>

<h3 id="案例2：爬取电商网站产品信息"><a href="#案例2：爬取电商网站产品信息" class="headerlink" title="案例2：爬取电商网站产品信息"></a>案例2：爬取电商网站产品信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EcommerceSpider</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.base_url = <span class="string">&#x27;https://www.example-shop.com/products&#x27;</span></span><br><span class="line">        <span class="variable language_">self</span>.headers = &#123;</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="variable language_">self</span>.products = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_page</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;获取页面内容&quot;&quot;&quot;</span></span><br><span class="line">        time.sleep(random.uniform(<span class="number">1</span>, <span class="number">3</span>))  <span class="comment"># 随机延迟</span></span><br><span class="line">        response = requests.get(url, headers=<span class="variable language_">self</span>.headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;请求失败: <span class="subst">&#123;response.status_code&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_product_list</span>(<span class="params">self, soup</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;解析产品列表页&quot;&quot;&quot;</span></span><br><span class="line">        product_cards = soup.select(<span class="string">&#x27;.product-card&#x27;</span>)</span><br><span class="line">        product_links = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> card <span class="keyword">in</span> product_cards:</span><br><span class="line">            link = card.select_one(<span class="string">&#x27;a.product-link&#x27;</span>)[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> link.startswith(<span class="string">&#x27;http&#x27;</span>):</span><br><span class="line">                link = <span class="string">&#x27;https://www.example-shop.com&#x27;</span> + link</span><br><span class="line">            product_links.append(link)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> product_links</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_product_detail</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;解析产品详情页&quot;&quot;&quot;</span></span><br><span class="line">        soup = <span class="variable language_">self</span>.get_page(url)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> soup:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            product = &#123;</span><br><span class="line">                <span class="string">&#x27;url&#x27;</span>: url,</span><br><span class="line">                <span class="string">&#x27;name&#x27;</span>: soup.select_one(<span class="string">&#x27;h1.product-name&#x27;</span>).text.strip(),</span><br><span class="line">                <span class="string">&#x27;price&#x27;</span>: soup.select_one(<span class="string">&#x27;span.price&#x27;</span>).text.strip(),</span><br><span class="line">                <span class="string">&#x27;description&#x27;</span>: soup.select_one(<span class="string">&#x27;div.description&#x27;</span>).text.strip(),</span><br><span class="line">                <span class="string">&#x27;rating&#x27;</span>: soup.select_one(<span class="string">&#x27;div.rating&#x27;</span>).text.strip() <span class="keyword">if</span> soup.select_one(<span class="string">&#x27;div.rating&#x27;</span>) <span class="keyword">else</span> <span class="string">&#x27;No rating&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;reviews_count&#x27;</span>: soup.select_one(<span class="string">&#x27;span.reviews-count&#x27;</span>).text.strip() <span class="keyword">if</span> soup.select_one(<span class="string">&#x27;span.reviews-count&#x27;</span>) <span class="keyword">else</span> <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;availability&#x27;</span>: soup.select_one(<span class="string">&#x27;div.availability&#x27;</span>).text.strip() <span class="keyword">if</span> soup.select_one(<span class="string">&#x27;div.availability&#x27;</span>) <span class="keyword">else</span> <span class="string">&#x27;Unknown&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;images&#x27;</span>: [img[<span class="string">&#x27;src&#x27;</span>] <span class="keyword">for</span> img <span class="keyword">in</span> soup.select(<span class="string">&#x27;div.product-images img&#x27;</span>)],</span><br><span class="line">                <span class="string">&#x27;specifications&#x27;</span>: &#123;&#125;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 提取规格</span></span><br><span class="line">            specs_table = soup.select_one(<span class="string">&#x27;table.specifications&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> specs_table:</span><br><span class="line">                rows = specs_table.select(<span class="string">&#x27;tr&#x27;</span>)</span><br><span class="line">                <span class="keyword">for</span> row <span class="keyword">in</span> rows:</span><br><span class="line">                    cols = row.select(<span class="string">&#x27;td&#x27;</span>)</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">len</span>(cols) &gt;= <span class="number">2</span>:</span><br><span class="line">                        key = cols[<span class="number">0</span>].text.strip()</span><br><span class="line">                        value = cols[<span class="number">1</span>].text.strip()</span><br><span class="line">                        product[<span class="string">&#x27;specifications&#x27;</span>][key] = value</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">return</span> product</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;解析产品详情出错: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">scrape</span>(<span class="params">self, pages=<span class="number">3</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;爬取指定页数的产品&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, pages + <span class="number">1</span>):</span><br><span class="line">            page_url = <span class="string">f&quot;<span class="subst">&#123;self.base_url&#125;</span>?page=<span class="subst">&#123;page&#125;</span>&quot;</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;爬取页面: <span class="subst">&#123;page_url&#125;</span>&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            soup = <span class="variable language_">self</span>.get_page(page_url)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> soup:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            product_links = <span class="variable language_">self</span>.parse_product_list(soup)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;找到 <span class="subst">&#123;<span class="built_in">len</span>(product_links)&#125;</span> 个产品链接&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> link <span class="keyword">in</span> product_links:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;爬取产品: <span class="subst">&#123;link&#125;</span>&quot;</span>)</span><br><span class="line">                product = <span class="variable language_">self</span>.parse_product_detail(link)</span><br><span class="line">                <span class="keyword">if</span> product:</span><br><span class="line">                    <span class="variable language_">self</span>.products.append(product)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 保存结果</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;products.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            json.dump(<span class="variable language_">self</span>.products, f, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">4</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;已爬取 <span class="subst">&#123;<span class="built_in">len</span>(self.products)&#125;</span> 个产品并保存到 products.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    spider = EcommerceSpider()</span><br><span class="line">    spider.scrape(pages=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<h3 id="案例3：使用Scrapy爬取GitHub仓库信息"><a href="#案例3：使用Scrapy爬取GitHub仓库信息" class="headerlink" title="案例3：使用Scrapy爬取GitHub仓库信息"></a>案例3：使用Scrapy爬取GitHub仓库信息</h3><p>首先创建Scrapy项目：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject github_scraper</span><br><span class="line"><span class="built_in">cd</span> github_scraper</span><br><span class="line">scrapy genspider repos github.com</span><br></pre></td></tr></table></figure>

<p>编辑爬虫文件 <code>github_scraper/spiders/repos.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> FormRequest</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ReposSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;repos&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;github.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://github.com/login&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 提取CSRF令牌</span></span><br><span class="line">        token = response.css(<span class="string">&#x27;input[name=&quot;authenticity_token&quot;]::attr(value)&#x27;</span>).get()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 提交登录表单</span></span><br><span class="line">        <span class="keyword">return</span> FormRequest.from_response(</span><br><span class="line">            response,</span><br><span class="line">            formdata=&#123;</span><br><span class="line">                <span class="string">&#x27;login&#x27;</span>: <span class="string">&#x27;your_username&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;your_password&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;authenticity_token&#x27;</span>: token</span><br><span class="line">            &#125;,</span><br><span class="line">            callback=<span class="variable language_">self</span>.after_login</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">after_login</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 检查登录是否成功</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;Sign out&#x27;</span> <span class="keyword">in</span> response.text:</span><br><span class="line">            <span class="variable language_">self</span>.log(<span class="string">&quot;登录成功!&quot;</span>)</span><br><span class="line">            <span class="comment"># 访问Python组织的仓库页面</span></span><br><span class="line">            <span class="keyword">return</span> scrapy.Request(<span class="string">&#x27;https://github.com/python&#x27;</span>, callback=<span class="variable language_">self</span>.parse_org)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.log(<span class="string">&quot;登录失败!&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_org</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 访问仓库标签页</span></span><br><span class="line">        repos_url = response.css(<span class="string">&#x27;a[data-tab-item=&quot;repositories&quot;]::attr(href)&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">return</span> response.follow(repos_url, callback=<span class="variable language_">self</span>.parse_repos)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_repos</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 提取仓库信息</span></span><br><span class="line">        <span class="keyword">for</span> repo <span class="keyword">in</span> response.css(<span class="string">&#x27;li.Box-row&#x27;</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">&#x27;name&#x27;</span>: repo.css(<span class="string">&#x27;a[itemprop=&quot;name codeRepository&quot;]::text&#x27;</span>).get().strip(),</span><br><span class="line">                <span class="string">&#x27;description&#x27;</span>: repo.css(<span class="string">&#x27;p[itemprop=&quot;description&quot;]::text&#x27;</span>).get(<span class="string">&#x27;&#x27;</span>).strip(),</span><br><span class="line">                <span class="string">&#x27;language&#x27;</span>: repo.css(<span class="string">&#x27;span[itemprop=&quot;programmingLanguage&quot;]::text&#x27;</span>).get(<span class="string">&#x27;&#x27;</span>).strip(),</span><br><span class="line">                <span class="string">&#x27;stars&#x27;</span>: repo.css(<span class="string">&#x27;a.Link--muted[href$=&quot;/stargazers&quot;]::text&#x27;</span>).get(<span class="string">&#x27;&#x27;</span>).strip(),</span><br><span class="line">                <span class="string">&#x27;forks&#x27;</span>: repo.css(<span class="string">&#x27;a.Link--muted[href$=&quot;/forks&quot;]::text&#x27;</span>).get(<span class="string">&#x27;&#x27;</span>).strip(),</span><br><span class="line">                <span class="string">&#x27;updated&#x27;</span>: repo.css(<span class="string">&#x27;relative-time::attr(datetime)&#x27;</span>).get(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 处理分页</span></span><br><span class="line">        next_page = response.css(<span class="string">&#x27;a.next_page::attr(href)&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page:</span><br><span class="line">            <span class="keyword">yield</span> response.follow(next_page, callback=<span class="variable language_">self</span>.parse_repos)</span><br></pre></td></tr></table></figure>

<p>创建项目设置文件 <code>github_scraper/settings.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加以下设置</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">2</span></span><br><span class="line">COOKIES_ENABLED = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加User-Agent</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&#x27;</span></span><br></pre></td></tr></table></figure>

<p>运行爬虫：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl repos -o github_repos.json</span><br></pre></td></tr></table></figure>

<h2 id="爬虫进阶技巧"><a href="#爬虫进阶技巧" class="headerlink" title="爬虫进阶技巧"></a>爬虫进阶技巧</h2><h3 id="1-使用异步爬虫提高效率"><a href="#1-使用异步爬虫提高效率" class="headerlink" title="1. 使用异步爬虫提高效率"></a>1. 使用异步爬虫提高效率</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">fetch</span>(<span class="params">session, url</span>):</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> response:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">await</span> response.text()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">html</span>):</span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">    title = soup.title.string <span class="keyword">if</span> soup.title <span class="keyword">else</span> <span class="string">&quot;No title&quot;</span></span><br><span class="line">    <span class="keyword">return</span> title</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">scrape</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        html = <span class="keyword">await</span> fetch(session, url)</span><br><span class="line">        title = <span class="keyword">await</span> parse(html)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;url&#125;</span> - <span class="subst">&#123;title&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    urls = [</span><br><span class="line">        <span class="string">&#x27;https://www.example.com&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.example.org&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.example.net&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.example.edu&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.example.io&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line">    tasks = [scrape(url) <span class="keyword">for</span> url <span class="keyword">in</span> urls]</span><br><span class="line">    <span class="keyword">await</span> asyncio.gather(*tasks)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    asyncio.run(main())</span><br></pre></td></tr></table></figure>

<h3 id="2-使用IP代理池"><a href="#2-使用IP代理池" class="headerlink" title="2. 使用IP代理池"></a>2. 使用IP代理池</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ProxyManager</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.proxies = []</span><br><span class="line">        <span class="variable language_">self</span>.current_proxy = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.max_failures = <span class="number">3</span></span><br><span class="line">        <span class="variable language_">self</span>.failure_count = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_proxy_list</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;从代理网站获取代理列表&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            response = requests.get(<span class="string">&#x27;https://www.free-proxy-list.net/&#x27;</span>)</span><br><span class="line">            soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">            table = soup.find(<span class="string">&#x27;table&#x27;</span>, &#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;proxylisttable&#x27;</span>&#125;)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> row <span class="keyword">in</span> table.tbody.find_all(<span class="string">&#x27;tr&#x27;</span>):</span><br><span class="line">                cols = row.find_all(<span class="string">&#x27;td&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> cols[<span class="number">6</span>].text.strip() == <span class="string">&#x27;yes&#x27;</span>:  <span class="comment"># HTTPS代理</span></span><br><span class="line">                    proxy = &#123;</span><br><span class="line">                        <span class="string">&#x27;ip&#x27;</span>: cols[<span class="number">0</span>].text.strip(),</span><br><span class="line">                        <span class="string">&#x27;port&#x27;</span>: cols[<span class="number">1</span>].text.strip(),</span><br><span class="line">                        <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://&#x27;</span> + cols[<span class="number">0</span>].text.strip() + <span class="string">&#x27;:&#x27;</span> + cols[<span class="number">1</span>].text.strip()</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="variable language_">self</span>.proxies.append(proxy)</span><br><span class="line">            </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;获取到 <span class="subst">&#123;<span class="built_in">len</span>(self.proxies)&#125;</span> 个代理&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;获取代理列表失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_random_proxy</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;获取随机代理&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.proxies:</span><br><span class="line">            <span class="variable language_">self</span>.get_proxy_list()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.proxies:</span><br><span class="line">            <span class="variable language_">self</span>.current_proxy = random.choice(<span class="variable language_">self</span>.proxies)</span><br><span class="line">            <span class="keyword">return</span> &#123;</span><br><span class="line">                <span class="string">&#x27;https&#x27;</span>: <span class="variable language_">self</span>.current_proxy[<span class="string">&#x27;https&#x27;</span>]</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">handle_request_error</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;处理请求错误&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.failure_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.failure_count &gt;= <span class="variable language_">self</span>.max_failures:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.current_proxy <span class="keyword">in</span> <span class="variable language_">self</span>.proxies:</span><br><span class="line">                <span class="variable language_">self</span>.proxies.remove(<span class="variable language_">self</span>.current_proxy)</span><br><span class="line">            <span class="variable language_">self</span>.current_proxy = <span class="literal">None</span></span><br><span class="line">            <span class="variable language_">self</span>.failure_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用代理管理器</span></span><br><span class="line">proxy_manager = ProxyManager()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scrape_with_proxy</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用代理爬取网页&quot;&quot;&quot;</span></span><br><span class="line">    max_retries = <span class="number">5</span></span><br><span class="line">    retries = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> retries &lt; max_retries:</span><br><span class="line">        proxy = proxy_manager.get_random_proxy()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> proxy:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;没有可用代理&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;使用代理: <span class="subst">&#123;proxy[<span class="string">&#x27;https&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line">            response = requests.get(url, proxies=proxy, timeout=<span class="number">10</span>)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;请求失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">            proxy_manager.handle_request_error()</span><br><span class="line">        </span><br><span class="line">        retries += <span class="number">1</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;所有重试都失败了&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<h3 id="3-使用User-Agent池"><a href="#3-使用User-Agent池" class="headerlink" title="3. 使用User-Agent池"></a>3. 使用User-Agent池</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UserAgentManager</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.user_agents = [</span><br><span class="line">            <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59&#x27;</span></span><br><span class="line">        ]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_random_user_agent</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;获取随机User-Agent&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> random.choice(<span class="variable language_">self</span>.user_agents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用User-Agent管理器</span></span><br><span class="line">ua_manager = UserAgentManager()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scrape_with_random_ua</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用随机User-Agent爬取网页&quot;&quot;&quot;</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: ua_manager.get_random_user_agent()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        <span class="keyword">return</span> response.text</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;请求失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<h3 id="4-使用爬虫调度器"><a href="#4-使用爬虫调度器" class="headerlink" title="4. 使用爬虫调度器"></a>4. 使用爬虫调度器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Scheduler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_threads=<span class="number">5</span>, delay=<span class="number">2</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.queue = queue.Queue()</span><br><span class="line">        <span class="variable language_">self</span>.results = []</span><br><span class="line">        <span class="variable language_">self</span>.num_threads = num_threads</span><br><span class="line">        <span class="variable language_">self</span>.delay = delay</span><br><span class="line">        <span class="variable language_">self</span>.lock = threading.Lock()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_task</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;添加爬取任务&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.queue.put(url)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">worker</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;工作线程&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            url = <span class="variable language_">self</span>.queue.get()</span><br><span class="line">            <span class="keyword">if</span> url <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;爬取: <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line">                response = requests.get(url)</span><br><span class="line">                <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                    soup = BeautifulSoup(response.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">                    title = soup.title.string <span class="keyword">if</span> soup.title <span class="keyword">else</span> <span class="string">&quot;No title&quot;</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">with</span> <span class="variable language_">self</span>.lock:</span><br><span class="line">                        <span class="variable language_">self</span>.results.append(&#123;</span><br><span class="line">                            <span class="string">&#x27;url&#x27;</span>: url,</span><br><span class="line">                            <span class="string">&#x27;title&#x27;</span>: title,</span><br><span class="line">                            <span class="string">&#x27;status&#x27;</span>: response.status_code</span><br><span class="line">                        &#125;)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;爬取 <span class="subst">&#123;url&#125;</span> 失败: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="variable language_">self</span>.queue.task_done()</span><br><span class="line">            time.sleep(<span class="variable language_">self</span>.delay)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;运行调度器&quot;&quot;&quot;</span></span><br><span class="line">        threads = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建工作线程</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_threads):</span><br><span class="line">            thread = threading.Thread(target=<span class="variable language_">self</span>.worker)</span><br><span class="line">            thread.start()</span><br><span class="line">            threads.append(thread)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 等待队列处理完成</span></span><br><span class="line">        <span class="variable language_">self</span>.queue.join()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 停止工作线程</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_threads):</span><br><span class="line">            <span class="variable language_">self</span>.queue.put(<span class="literal">None</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> thread <span class="keyword">in</span> threads:</span><br><span class="line">            thread.join()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用调度器</span></span><br><span class="line">scheduler = Scheduler(num_threads=<span class="number">3</span>, delay=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加任务</span></span><br><span class="line">urls = [</span><br><span class="line">    <span class="string">&#x27;https://www.example.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https://www.example.org&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https://www.example.net&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https://www.example.edu&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https://www.example.io&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https://www.example.dev&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https://www.example.app&#x27;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">    scheduler.add_task(url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行调度器</span></span><br><span class="line">results = scheduler.run()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;爬取了 <span class="subst">&#123;<span class="built_in">len</span>(results)&#125;</span> 个网页&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;result[<span class="string">&#x27;url&#x27;</span>]&#125;</span> - <span class="subst">&#123;result[<span class="string">&#x27;title&#x27;</span>]&#125;</span> (状态码: <span class="subst">&#123;result[<span class="string">&#x27;status&#x27;</span>]&#125;</span>)&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="爬虫的法律和道德问题"><a href="#爬虫的法律和道德问题" class="headerlink" title="爬虫的法律和道德问题"></a>爬虫的法律和道德问题</h2><h3 id="法律考量"><a href="#法律考量" class="headerlink" title="法律考量"></a>法律考量</h3><ol>
<li><strong>遵守网站的服务条款</strong>：许多网站在服务条款中明确禁止爬虫</li>
<li><strong>尊重robots.txt</strong>：这是网站告诉爬虫哪些页面可以爬取的标准</li>
<li><strong>版权法</strong>：爬取的内容可能受版权保护</li>
<li><strong>数据保护法规</strong>：如果爬取个人数据，需要遵守GDPR等数据保护法规</li>
<li><strong>计算机滥用法</strong>：过度爬取可能被视为对服务器的攻击</li>
</ol>
<h3 id="道德考量"><a href="#道德考量" class="headerlink" title="道德考量"></a>道德考量</h3><ol>
<li><strong>不要对网站造成负担</strong>：控制请求频率</li>
<li><strong>识别你的爬虫</strong>：在User-Agent中标明你的爬虫身份</li>
<li><strong>缓存数据</strong>：避免重复请求相同的内容</li>
<li><strong>尊重隐私</strong>：不要爬取和存储个人敏感信息</li>
<li><strong>考虑使用API</strong>：如果网站提供API，优先使用API而不是爬虫</li>
</ol>
<h3 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h3><ol>
<li><strong>阅读网站的服务条款和robots.txt</strong></li>
<li><strong>控制爬取速度</strong>：使用延迟和限速</li>
<li><strong>缓存结果</strong>：避免重复请求</li>
<li><strong>处理错误</strong>：优雅地处理异常和错误</li>
<li><strong>监控爬虫</strong>：确保它按预期工作</li>
<li><strong>定期更新爬虫</strong>：网站结构可能会改变</li>
</ol>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>网络爬虫是一个强大的工具，可以帮助我们自动化地从网络上获取数据。Python提供了丰富的库和框架，使爬虫开发变得相对简单。从基本的requests和BeautifulSoup，到高级的Scrapy框架，再到处理JavaScript渲染页面的Selenium，Python生态系统为各种爬虫需求提供了解决方案。</p>
<p>然而，使用爬虫时，我们必须记住法律和道德责任。尊重网站的服务条款，遵守robots.txt，控制爬取速度，这些都是负责任的爬虫行为。</p>
<p>希望这篇文章能帮助你理解网络爬虫的基础知识和高级技术，并在实际项目中负责任地应用这些技术。</p>
<p>你有什么关于Python网络爬虫的问题或经验分享吗？欢迎在评论中讨论！</p>

                    
                </div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                            <ul class="post-tags-box border-box">
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a>
                                    </li>
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/">数据采集</a>
                                    </li>
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/requests/">requests</a>
                                    </li>
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/BeautifulSoup/">BeautifulSoup</a>
                                    </li>
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/Scrapy/">Scrapy</a>
                                    </li>
                                
                            </ul>
                        
                    </div>
                    <div>
                        
                    </div>
                </div>

                

                
                    <div class="post-nav border-box">
                        
                            <div class="prev-post">
                                <a class="prev"
                                   rel="prev"
                                   href="/2023/python/python-decorators-practical-examples/"
                                   title="Python装饰器实战：15个实用案例详解"
                                >
                                    <span class="left arrow-icon flex-center">
                                        <i class="fas fa-chevron-left"></i>
                                    </span>
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">Python装饰器实战：15个实用案例详解</span>
                                        <span class="post-nav-item">上一篇</span>
                                    </span>
                                </a>
                            </div>
                        
                        
                            <div class="next-post">
                                <a class="next"
                                   rel="next"
                                   href="/2022/php/php82-dnf-types-guide/"
                                   title="PHP 8.2 析取范式类型深度解析：复杂类型系统的新突破"
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">PHP 8.2 析取范式类型深度解析：复杂类型系统的新突破</span>
                                        <span class="post-nav-item">下一篇</span>
                                    </span>
                                    <span class="right arrow-icon flex-center">
                                        <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    






                
            </div>
        </div>

        
            <div class="pc-post-toc right-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%EF%BC%9A%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A"><span class="nav-text">Python网络爬虫技术：从入门到精通</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80"><span class="nav-text">网络爬虫基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%9F"><span class="nav-text">什么是网络爬虫？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E5%90%88%E6%B3%95%E6%80%A7%E5%92%8C%E9%81%93%E5%BE%B7%E8%80%83%E9%87%8F"><span class="nav-text">爬虫的合法性和道德考量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="nav-text">网络爬虫的基本流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E7%88%AC%E8%99%AB%E5%B7%A5%E5%85%B7"><span class="nav-text">基本爬虫工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Requests%E5%BA%93"><span class="nav-text">Requests库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BeautifulSoup%E5%BA%93"><span class="nav-text">BeautifulSoup库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lxml%E5%BA%93"><span class="nav-text">lxml库</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AD%E7%BA%A7%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF"><span class="nav-text">中级爬虫技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E8%A1%A8%E5%8D%95%E5%92%8C%E7%99%BB%E5%BD%95"><span class="nav-text">处理表单和登录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86Cookie"><span class="nav-text">处理Cookie</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86JavaScript%E6%B8%B2%E6%9F%93%E7%9A%84%E9%A1%B5%E9%9D%A2"><span class="nav-text">处理JavaScript渲染的页面</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Selenium"><span class="nav-text">使用Selenium</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Requests-HTML"><span class="nav-text">使用Requests-HTML</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%88%86%E9%A1%B5"><span class="nav-text">处理分页</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E7%BA%A7%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF"><span class="nav-text">高级爬虫技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6"><span class="nav-text">使用Scrapy框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%8F%8D%E7%88%AC%E8%99%AB%E6%9C%BA%E5%88%B6"><span class="nav-text">处理反爬虫机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%AE%BE%E7%BD%AE%E8%AF%B7%E6%B1%82%E5%A4%B4"><span class="nav-text">1. 设置请求头</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%8E%A7%E5%88%B6%E8%AF%B7%E6%B1%82%E9%A2%91%E7%8E%87"><span class="nav-text">2. 控制请求频率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86"><span class="nav-text">3. 使用代理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E5%A4%84%E7%90%86%E9%AA%8C%E8%AF%81%E7%A0%81"><span class="nav-text">4. 处理验证码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="nav-text">数据存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BF%9D%E5%AD%98%E4%B8%BACSV"><span class="nav-text">1. 保存为CSV</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%BF%9D%E5%AD%98%E4%B8%BAJSON"><span class="nav-text">2. 保存为JSON</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BF%9D%E5%AD%98%E5%88%B0%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-text">3. 保存到数据库</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%99%85%E7%88%AC%E8%99%AB%E6%A1%88%E4%BE%8B"><span class="nav-text">实际爬虫案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B1%EF%BC%9A%E7%88%AC%E5%8F%96%E6%96%B0%E9%97%BB%E7%BD%91%E7%AB%99"><span class="nav-text">案例1：爬取新闻网站</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B2%EF%BC%9A%E7%88%AC%E5%8F%96%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E4%BA%A7%E5%93%81%E4%BF%A1%E6%81%AF"><span class="nav-text">案例2：爬取电商网站产品信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B3%EF%BC%9A%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96GitHub%E4%BB%93%E5%BA%93%E4%BF%A1%E6%81%AF"><span class="nav-text">案例3：使用Scrapy爬取GitHub仓库信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E8%BF%9B%E9%98%B6%E6%8A%80%E5%B7%A7"><span class="nav-text">爬虫进阶技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BD%BF%E7%94%A8%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98%E6%95%88%E7%8E%87"><span class="nav-text">1. 使用异步爬虫提高效率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BD%BF%E7%94%A8IP%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-text">2. 使用IP代理池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%BD%BF%E7%94%A8User-Agent%E6%B1%A0"><span class="nav-text">3. 使用User-Agent池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%BD%BF%E7%94%A8%E7%88%AC%E8%99%AB%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="nav-text">4. 使用爬虫调度器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E6%B3%95%E5%BE%8B%E5%92%8C%E9%81%93%E5%BE%B7%E9%97%AE%E9%A2%98"><span class="nav-text">爬虫的法律和道德问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%95%E5%BE%8B%E8%80%83%E9%87%8F"><span class="nav-text">法律考量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%81%93%E5%BE%B7%E8%80%83%E9%87%8F"><span class="nav-text">道德考量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="nav-text">最佳实践</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-text">结论</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>
        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="copyright-info info-item">
    &copy;&nbsp;2025
    
            &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/">Orion K</a>
        
    </div>


    
        
        <div class="deploy-info info-item">
            
            本站由 <span class="tooltip" data-tooltip-content="GitHub Pages"><img src="/images/brands/github.png"></span> 提供部署服务
            
        </div>
    

    

    
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools right-toc">
            <div class="post-tools-container border-box">
    <ul class="post-tools-list border-box">
        <!-- PC encrypt again -->
        

        <!-- PC TOC show toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- PC go comment -->
        

        <!-- PC full screen -->
        <li class="tools-item flex-center full-screen">
            <i class="fa-solid fa-expand"></i>
        </li>
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <!-- toggle mode -->
        
            <li class="tools-item tool-toggle-theme-mode flex-center">
                <i class="fas fa-moon"></i>
            </li>
        

        <!-- rss -->
        

        <!-- to bottom -->
        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        
            <li class="tools-item toggle-show-toc-tablet flex-center">
                <i class="fas fa-list"></i>
            </li>
        

        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

    <!-- tablet toc -->
    
        <div class="tablet-post-toc-mask">
            <div class="tablet-post-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF%EF%BC%9A%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A"><span class="nav-text">Python网络爬虫技术：从入门到精通</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80"><span class="nav-text">网络爬虫基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%9F"><span class="nav-text">什么是网络爬虫？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E5%90%88%E6%B3%95%E6%80%A7%E5%92%8C%E9%81%93%E5%BE%B7%E8%80%83%E9%87%8F"><span class="nav-text">爬虫的合法性和道德考量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="nav-text">网络爬虫的基本流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E7%88%AC%E8%99%AB%E5%B7%A5%E5%85%B7"><span class="nav-text">基本爬虫工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Requests%E5%BA%93"><span class="nav-text">Requests库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BeautifulSoup%E5%BA%93"><span class="nav-text">BeautifulSoup库</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lxml%E5%BA%93"><span class="nav-text">lxml库</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AD%E7%BA%A7%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF"><span class="nav-text">中级爬虫技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E8%A1%A8%E5%8D%95%E5%92%8C%E7%99%BB%E5%BD%95"><span class="nav-text">处理表单和登录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86Cookie"><span class="nav-text">处理Cookie</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86JavaScript%E6%B8%B2%E6%9F%93%E7%9A%84%E9%A1%B5%E9%9D%A2"><span class="nav-text">处理JavaScript渲染的页面</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Selenium"><span class="nav-text">使用Selenium</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Requests-HTML"><span class="nav-text">使用Requests-HTML</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%88%86%E9%A1%B5"><span class="nav-text">处理分页</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E7%BA%A7%E7%88%AC%E8%99%AB%E6%8A%80%E6%9C%AF"><span class="nav-text">高级爬虫技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Scrapy%E6%A1%86%E6%9E%B6"><span class="nav-text">使用Scrapy框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%8F%8D%E7%88%AC%E8%99%AB%E6%9C%BA%E5%88%B6"><span class="nav-text">处理反爬虫机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%AE%BE%E7%BD%AE%E8%AF%B7%E6%B1%82%E5%A4%B4"><span class="nav-text">1. 设置请求头</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%8E%A7%E5%88%B6%E8%AF%B7%E6%B1%82%E9%A2%91%E7%8E%87"><span class="nav-text">2. 控制请求频率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86"><span class="nav-text">3. 使用代理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E5%A4%84%E7%90%86%E9%AA%8C%E8%AF%81%E7%A0%81"><span class="nav-text">4. 处理验证码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="nav-text">数据存储</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BF%9D%E5%AD%98%E4%B8%BACSV"><span class="nav-text">1. 保存为CSV</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%BF%9D%E5%AD%98%E4%B8%BAJSON"><span class="nav-text">2. 保存为JSON</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BF%9D%E5%AD%98%E5%88%B0%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-text">3. 保存到数据库</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%99%85%E7%88%AC%E8%99%AB%E6%A1%88%E4%BE%8B"><span class="nav-text">实际爬虫案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B1%EF%BC%9A%E7%88%AC%E5%8F%96%E6%96%B0%E9%97%BB%E7%BD%91%E7%AB%99"><span class="nav-text">案例1：爬取新闻网站</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B2%EF%BC%9A%E7%88%AC%E5%8F%96%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E4%BA%A7%E5%93%81%E4%BF%A1%E6%81%AF"><span class="nav-text">案例2：爬取电商网站产品信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B3%EF%BC%9A%E4%BD%BF%E7%94%A8Scrapy%E7%88%AC%E5%8F%96GitHub%E4%BB%93%E5%BA%93%E4%BF%A1%E6%81%AF"><span class="nav-text">案例3：使用Scrapy爬取GitHub仓库信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E8%BF%9B%E9%98%B6%E6%8A%80%E5%B7%A7"><span class="nav-text">爬虫进阶技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BD%BF%E7%94%A8%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB%E6%8F%90%E9%AB%98%E6%95%88%E7%8E%87"><span class="nav-text">1. 使用异步爬虫提高效率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BD%BF%E7%94%A8IP%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-text">2. 使用IP代理池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%BD%BF%E7%94%A8User-Agent%E6%B1%A0"><span class="nav-text">3. 使用User-Agent池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%BD%BF%E7%94%A8%E7%88%AC%E8%99%AB%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="nav-text">4. 使用爬虫调度器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E6%B3%95%E5%BE%8B%E5%92%8C%E9%81%93%E5%BE%B7%E9%97%AE%E9%A2%98"><span class="nav-text">爬虫的法律和道德问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%95%E5%BE%8B%E8%80%83%E9%87%8F"><span class="nav-text">法律考量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%81%93%E5%BE%B7%E8%80%83%E9%87%8F"><span class="nav-text">道德考量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="nav-text">最佳实践</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-text">结论</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        </div>
    
</main>





<!-- common js -->

<script src="/js/utils.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/toggle-theme.js"></script>

<script src="/js/code-block.js"></script>

<script src="/js/main.js"></script>

<script src="/js/libs/anime.min.js"></script>


<!-- local search -->

    
<script src="/js/local-search.js"></script>



<!-- lazyload -->


<div class="">
    <!-- home page -->
    

    <!-- post page -->
    
        <!-- post-helper -->
        
<script src="/js/post/post-helper.js"></script>


        <!-- toc -->
        
            
<script src="/js/post/toc.js"></script>

        

        <!-- copyright-info -->
        

        <!-- share -->
        
    

    <!-- categories page -->
    

    <!-- links page -->
    

    <!-- photos page -->
    

    <!-- tools page -->
    
</div>

<!-- mermaid -->


<!-- pjax -->



</body>
</html>
